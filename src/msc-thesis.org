#+setupfile: ./tex-setup.org
#+startup: latexpreview show2levels
#+author: Vincent C. Mader
#+title: Computational Cost Optimization via Stochastic Kernel Sampling for the Numerical Integration of the Smoluchovski Coagulation Equation
\newpage
#+latex: \listoffigures
\newpage
#+latex: \listoftables
\newpage
* Preface
** Acknowledgements
\newpage
** Abstract
_What is the topic of this thesis?_
- We want to =test whether= the numerical =integration=
  of the Smoluchovski =coagulation equation= can be =sped up=
  via =stochastic sampling= of the kernel.
- Depending on how accurate one wants to model the involved processes,
  the integration of the coagulation equation can become quite costly
  (from a computational standpoint).
- This is due to the "high dimensionality" of the problem.
  + 3 dimensions of space (in principle)
  + 3 dimensions of mass (explain why -> fragmentation)
  + X dimensions of porosity
  + + maybe other attributes of the dust particles
- Therefore, the idea is that a big gain in computational performance
  could be gained by only including the "relevant" parts of the kernel
  into the integration. ("relevant" according to what criterion?)
- Finding out whether this works
  (and by =how much= the computational cost can be reduced)
  is the big goal of this thesis.
\newpage
* Introduction
* Definition of a Simplified Dust Coagulation Model
** Prerequisites and Assumptions
Consider a distribution of dust particles with different masses $m$. For simplicity,
all of them are assumed to share the same material density $\rho_s$, as well as all other
properties except their particle mass and radius.

In this highly simplified model, the particles are furthermore assumed to be spherically
shaped, such that the relationship between mass $m$ and radius $a$ can be expressed as
\begin{align*}
  m=\frac{4\pi}{3}\rho_s a^3
\end{align*}
** The Dust Particle Mass Distribution Function
Let $n(m)$ label the /particle mass distribution function/, i.e. the number of particles
per unit volume that possess a given mass value $m$. This distribution function shall be
defined in such a way that the total number $N$ of particles per unit volume is given by
\begin{align*}
  N=\int_0^\infty n(m)\ \dd m
\end{align*}

The mass density $\rho_s$, i.e. the total mass per unit volume, can then be written as
\begin{align*}
  \rho_s=\int_0^\infty m\cdot n(m)\ \dd m
\end{align*}
** Dust Particle Collisions
The movement of particles inside a protoplanetary disk can be assumed to be partly systematic
(due to velocity contributions of e.g. Keplerian motion) and partly random (due to effects of
e.g. Brownian or turbulent motion). This leads to occasional collision events.

The probability of more than two particles being involved in the same collision is so small
that the rate of simultaneous interactions between three (or more) bodies can be neglected
entirely. Thus, in this model, all collisions are assumed to be two-body collisions.

When two particles collide, there can be a number of different collision outcomes that have
to be taken into consideration. A simple approach of classifying these outcomes is to group
them into the following three scenarios:

1. /Bouncing/:
   Here, nothing really happens from the point of view of the particle mass distribution
   function. The particles collide, but do not exchange any mass. The collision itself of
   course /can/ have an effect on the particle kinematics via the exchange of momentum, but the
   distribution of masses stays unchanged. For that reason, this outcome does not have to be
   taken into account when modeling the evolution of the particle mass distribution.

2. /Merging/:
   The second case is more interesting for us: Here, the two colliding particles merge and form
   a single new particle. Thus, the two initial particles with mass values $m_1$ and $m_2$
   disappear, while a new particle with mass $m_1+m_2$ is created. The removal/addition of these
   masses from/to the mass distribution has to be handled in some fitting way, and we will take
   a look at how to do this in the next section.

3. /Fragmentation/:
   In the third case, the two colliding particles do also "disappear" during the interaction.
   In contrast to the case of merging though, here it is possible that a whole series of
   particles is brought into existence. When two particles collide and fragment, various
   smaller particles with different mass values can be created, under the condition that the
   sum of particle masses is equal to the total mass $m_1+m_2$ before the interaction.

To summarize: In this model, the input to a collision event is assumed to /always/ be two
particles, while the output can be any (natural) number, from a single particle in the case
of merging, to very many particles in the case of fragmentation.
** The Smoluchovski Coagulation Equation
Let us now turn our attention towards the question of how to formulate a model for the
particle mass distribution's evolution in time. One such model is given by the following
integro-differental population balance equation:
\begin{align*}
  \deriv{n}{t}(m,t)=\int_0^\infty\int_0^\infty K(m,m',m'')\cdot n(m',t)\cdot n(m'',t)\ \dd m'\ \dd m''
\end{align*}
This equation is typically referred to as the /Smoluchovski equation/, due to its treatment by
Marian Smoluchovski in 1916 [cite]. It describes how the number $n$ of particles (per unit
volume) with mass $m$ is affected by collision events at time $t$ involving two particles
with masses $m'$ and $m''$.

The quantity $K(m,\ m', m'')$ is called the (coagulation) /kernel/. It encodes all the
information about the collision events, their rates, and their outcomes.
** The Coagulation Kernel
\newpage
* Discretization of the Dust Coagulation Model
** +Introduction/General Idea+
Due to the discrete nature of number representation on digital computers, it is necessary
for numerical treatment to move away from a continuous representation. Instead, a discrete
analogon has to be constructed for the mass grid, the evolution equations, as well as the
coagulation kernel.
** Discretization of the Mass Axis
*** +Introduction/General Idea+
We will start with the discretization of the mass axis: Here, the goal is to partition the
continuous range of particle masses $m$ present in the disk into a set of $\mathcal N_m\in\mathbb N$
intervals, which from now on will be referred to as mass grid "bins". Each of these bins can
be uniquely characterized by an index $i\in[0,\mathcal N_m-1]$, and its corresponding mass
value $m_i^\text{c}$, which are situated at the center of the bin (ergo the superfix "c").

To derive an expression for these mass values, let us first define the upper and lower
boundaries of the discrete mass grid. We shall label them $m_\text{max}$ and $m_\text{min}$,
respectively. Now, consider a collection of $\mathcal N_m+1$ appropriately spaced grid points
$m_i^\text{b}$, which will serve as the bin boundaries.

\begin{figure}[h!]
    \caption{Illustration of the discretized mass axis.}
    \begin{center}
        \begin{tikzpicture}
            \def\N{4}      % This is the number of cells drawn (to the left of "..." separator).
            \def\M{5}      % This is the number of boundary arrows drawn (left of "...").
            \def\W{1.5}    % This is a cell's width.
            \def\H{\W}     % This is a cell's height.
            \def\L{\W/4}   % This is an arrow's length.
            \def\P{\W/8}   % This is the padding between arrow & cell.
            \def\R{\W/32}  % This is the padding between arrow & text.

            % Draw continuous mass axis.
            \draw [|-to](-\W, 2.5*\H) -- (\N*\W+4*\W, 2.5*\H);
            \draw [-](0, 2.45*\H) -- (0, 2.55*\H);
            \draw [-](\N*\W+3*\W, 2.45*\H) -- (\N*\W+3*\W, 2.55*\H);
            \node[] at (-\W-2*\P, 2.5*\H) {$0$};
            \node[] at (\N*\W+4*\W+2*\P, 2.5*\H) {$m$};

            % Draw arrow from continuous to discretized mass axis.
            \node[] at (\N*\W/2+1.5*\W, 2.25*\H) {$\Downarrow$};

            % Draw cells...
            % ...to the left of "..." separator.
            \foreach \x in {0, ..., \N} {
                \draw[draw=black] (\x*\W, 0) rectangle ++(\W, \H);
            }
            % ...to the right of "..." separator.
            \draw[draw=black] (\N*\W+2*\W, 0) rectangle ++(\W, \H);

            % Draw crosses...
            % ...to the left of "..." separator.
            \foreach \x in {0, ..., \N} {
                \draw (\x*\W+\W/2, \H/2) node {\tiny x};
            }
            % ...to the right of "..." separator.
            \draw (\N*\W+2.5*\W, \H/2) node {\tiny x};

            % Draw "..." separator.
            \node[] at (\N*\W+1.5*\W, \H/2) {...};

            % Draw arrows labeling cell boundaries.
            \foreach \x in {0, ..., \M} {
                \draw [-to](\x*\W, \H+\L+\P) -- (\x*\W, \H+\P);
                \node[] at (\x*\W, \H+\L+\P+\L+\R) {$m_\x^\text{b}$};
            }
            \draw [-to](\N*\W+2*\W, \H+\L+\P) -- (\N*\W+2*\W, \H+\P);
            \draw [-to](\N*\W+3*\W, \H+\L+\P) -- (\N*\W+3*\W, \H+\P);
            \node[] at (\N*\W+2*\W, \H+\L+\P+\L+\R) {$m_{\mathcal N_m}^\text{b}$};
            \node[] at (\N*\W+3*\W, \H+\L+\P+\L+\R) {$m_{\mathcal N_m+1}^\text{b}$};

            % Draw arrows labeling cell centers.
            \foreach \x in {0, ..., \N} {
                \draw [-to](\x*\W+\W/2, 0-\L-\P) -- (\x*\W+\W/2, 0-\P);
                \node[] at (\x*\W+\W/2, 0-\L-\P-\L-\R) {$m_\x^\text{c}$};
            }
            \draw [-to](\N*\W+2.5*\W, 0-\L-\P) -- (\N*\W+2.5*\W, 0-\P);
            \node[] at (\N*\W+2.5*\W, 0-\L-\P-\L-\R) {$m_{\mathcal N_m}^\text{c}$};

            % Draw labels for `m_min` and `m_max`.
            \node[] at (0, \H+\L+\P+\L+3*\R+\L+\L) {$m_\text{min}$};
            \node[] at (\N*\W+3*\W, \H+\L+\P+\L+3*\R+\L+\L) {$m_\text{max}$};
            \node[] at (0, \H+\L+\P+\L+2*\R+\L) {$=$};
            \node[] at (\N*\W+3*\W, \H+\L+\P+\L+2*\R+\L) {$=$};
        \end{tikzpicture}
    \end{center}
\end{figure}

*** Discretization of the Mass Axis using a Linear Scale
The spacing of these grid points of course depends heavily on the utilized
scaling, which for simplicity shall be assumed to be linear at the moment. Later on, we will
make use of a logarithmic scaling to assure a better representation of all masses along the
wide range of mass values present in the disk.

Analogously to the bin centers, the boundary points can be labeled by an index $i\in[0,\mathcal N_m]$.
The corresponding mass value $m_i^\text{b}$ can then be expressed as
\begin{align*}
  m_i^\text{b}=m_\text{min}+(m_\text{max}-m_\text{min})\cdot\frac{i}{\mathcal N_m}
\end{align*}

To calculate the the mass values for the grid points at the center of a bin, all we need to
do now is take the arithmetic mean of its two neighboring boundary values, i.e.:
\begin{align*}
  m_i^\text{c}=\frac{m_i^\text{b}+m_{i+1}^\text{b}}{2}
\end{align*}

Thus, after having defined only the three numbers $\mathcal N_m$, $m_\text{min}$, and $m_\text{max}$, it is
possible to interpolate the values of all mass grid points sitting on the boundaries
and/or centers of the bins.

\newpage

The inverse transformation from mass to index can be determined easily by rearranging the
above relation. This leads to the following expression
\begin{align*}
  i(m)=\mathcal N_m\cdot\frac{m-m_\text{min}}{m_\text{max}-m_\text{min}}
\end{align*}

_Sidenote_: In the linearily scaled mass grid, the bin "width" is constant, and independent of
the bin index. This will change once the switch to a logarithmic grid is made! For now
though, it is given by
\begin{align*}
  \Delta m
  &=m_{i+1}^\text{b}-m_i^\text{b}\\
  &=\frac{m_\text{max}-m_\text{min}}{\mathcal N_m}
\end{align*}
\newpage

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-0.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-1.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-2.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-3.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}
    \caption{CAPTION ...}
\end{figure}

\newpage

*** Discretization of the Mass Axis using a Logarithmic Scale
As before, let $\mathcal N_m$ be used to label the number of bins, and let the upper and lower grid
boundaries be given by $m_\text{min}$ and $m_\text{max}$, respectively.

Once again, we first define an expression for the grid points sitting directly on the bin
boundaries. Making use of an index $i\in\mathcal[0,N_m]$, they can be uniquely identified and
calculated via
\begin{align*}
  m_i^\text{b}=m_\text{min}\cdot\left(\frac{m_\text{max}}{m_\text{min}}\right)^{i/\mathcal N_m}
\end{align*}
To arrive at the mass values at the bin centers, we again take the mean. Here, it's not
the arithmetic mean though, but the geometric mean, i.e.
\begin{align*}
  m_i^\text{c}=\sqrt{m_i\cdot m_{i+1}}
\end{align*}

In contrast to the linear grid, the bin "width", i.e. the additive offset from one bin to the
next, is not the same for all bins in the logarithmic grid. Instead, what stays the same for
is the /relative/ mass increase from one bin to the next.

As above in the linear case, the inverse transformation can be arrived at by rearranging
for $i$. This leads to the following expression:
\begin{align*}
  i(m)
  =\mathcal N_m\cdot\frac{\log(m)-\log(m_\text{min})}{\log(m_\text{max})-\log(m_\text{min})}
\end{align*}

\newpage

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-0.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-1.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-2.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{
            ../../msc-thesis.py/figures/11/discrete-mass-axis_lin-scale-3.pdf
        }
        \subcaption{CAPTION ...}
    \end{minipage}
    \caption{CAPTION ...}
\end{figure}

\newpage

** Discretization of the Particle Mass Distribution Function
Just as we discretized the range of masses, so we also have
to discretize the particle mass distribution function.
For this, we define:
\begin{align*}
  n_i:=n(m_i)
\end{align*}
The number of particles per unit unit volume in bin $i$ is then
calculated by integration over the particle mass distribution
function from the lower to the upper bin boundary:
\begin{align*}
  N_i=\int_{m_{i-1/2}}^{m_{1+1/2}}n(m)\ \dd m
\end{align*}
The mass density $\rho_i$ in the bin $i$ is given by
\begin{align*}
  \rho_i=\int_{m_{i-1/2}}^{m_{1+1/2}}m\cdot n(m)\ \dd m
\end{align*}
To arrive at the total mass density independent of particle
mass, one has to sum over all bins:
\begin{align*}
  \rho=\sum_{i=0}^{\mathcal N_m-1}\rho_i
\end{align*}
** Discretization of the Smoluchovski Coagulation Equation

With $i,j\in\mathbb N$ "being indices":

\begin{align*}
  \deriv{n_k}{t}
    =\sum_{i=1}^{\mathcal N_m}\sum_{j=1}^{\mathcal N_m}
    K_{kij}\cdot n_i\cdot n_j
\end{align*}

Equivalent formulation: [which one to use? I like the lower one <3]

\begin{align*}
  \deriv{n_k}{t}
    =\sum_{i=0}^{\mathcal N_m-1}\sum_{j=0}^{\mathcal N_m-1}
    K_{kij}\cdot n_i\cdot n_j
\end{align*}

\newpage
* Initialization of the Particle Mass Distribution Function

Now, let us turn our attention to the initialization of the particle
mass distribution function.

_Options_:
1. Dirac-delta distribution
2. MRN distribution

What would be best?
- MRN is more physical than Dirac-delta. (see later chapter)
- It does not /really/ make that much of a difference though.
- The information about the initialization is quickly lost anyways.
- Both 1. & 2. qualitatively lead to the same results (even if not quantitatively).

_Observation_:
- Using the MRN distribution leads to a significant slowdown of the integration.
  (minutes, instead of seconds)
- This might be because I chose $m_\text{max}$ to be the largest mass grid bin.
  + Do not do this!
  + This is unphysical!
  + Instead use $\approx0.5\mum$
- Test whether this problem persists!

\newpage
* Integration of the Smoluchovski Equation
** How to discretize the time axis in a linear fashion?

Let the temporal domain be divided into $\mathcal N_t$ discrete "bins".
Then, let $\xi$ be the index used to uniquely identify each time step, i.e.

\begin{align*}
  \xi\in\mathbb N\cap[0,\mathcal N_m-1]
\end{align*}

When discretizing the time axis in a linear way, this leads to

\begin{align*}
  t_\xi=t_0+\xi\cdot\Delta t
\end{align*}

Here, $t_0$ labels the time value at the start of the simulation
(can be set to $t_0=0$ without loss of generality), and $\Delta t$
labels the size of each time step.

The step size needs to be chosen sufficiently small as to assure
stability of the integration scheme.

[...CFL criterion...]
[...implicit integration...]
** What is the difference between additive/multiplicative incrementation of time
- TODO: Move?
*** Additive time-steps
- straightforward approach
- incrementation of time via
\begin{align*}
  t_{n+1}=t_n+\Delta t
\end{align*}
*** Multiplicative time-steps
- more appropriate here [explain why]
\begin{align*}
  t_{n+1}=t_n\cdot q_t
\end{align*}

** How to integrate the Smoluchovski equation using an explicit Euler scheme?

As above, we use the notation $n_{i,\xi}$ to label the dust particle mass distribution
function evaluated for the mass $m_i$ at a point in time $t_\xi$.

The goal now is to compute the distribution function's value after
one additional step in time, i.e. at the time

\begin{align*}
  t_{\xi+1}=t_\xi+\Delta t
\end{align*}

This can be done by making use of an explicit Euler integration scheme [when?].
For a sufficiently small time step $\Delta t$, we can thus arrive at an approximate
expression for the distribution function's updated value via the transformation

\begin{align*}
  n_{k,\xi}
    &\to n_{k,\xi}
    +\Delta t\cdot\deriv{n_{k,\xi}}{t}\\
    &=n_{k,\xi}
    +\Delta t\cdot\sum_{i=0}^{\mathcal N_m-1}\sum_{j=0}^{\mathcal N_m-1}
    K_{kij}\cdot n_i\cdot n_j
\end{align*}

If the step size is chosen too large, the method will break down due to the
insufficient stability properties of the explicit Euler integration scheme.

Instead, [...implicit Euler/Radau...] [see above]
** How to discretize the time axis in a logarithmic fashion?

As the temporal domain spans over multiple orders of magnitude for
the processes involved in planet formation, it makes sense to discretize
the time axis in a logarithmic fashion. [write this differently?]

[...]
** How to integrate the Smoluchovski equation using implicit schemes?
\newpage
* Construction of the Kernel for Simple Hit-and-Stick Coagulation
** Introduction, Overview, & Goals for this Section
- Here, we only take a look at the simple case of...
  + ...simple /linear/ mass axis discretization.
  + ...simple /Dirac-delta/ initialization.
  + ...simple /explicit Euler/ integration.
*** Construction of a Kernel for Hit-and-Stick Coagulation
Assuming simple hit-and-stick coagulation, the kernel can be written as
\begin{equation*}
  K(m,m_1,m_2)
  =\frac{1}{2}\big[
    \delta(m-m_1-m_2)-\delta(m-m_1)-\delta(m-m_2)
  \big]\cdot R(m_1,m_2)
\end{equation*}
Here, the coefficient $R(m_1,m_2)$ labels the collision-and-merging rate.

\newpage
** Definition of the Kernel for simple Stick-and-Hit Coagulation

Let us first turn our attention to the simple case of stick-and-hit coagulation:
Here, two particles with masses $m_1$ and $m_2$ collide, and subsequently merge
into a single particle with combined mass $m_\text{tot}=m_1+m_2$.

Let $C(m_1,m_2)$ be the rate at which collisions occur between such a pair of particles.
Then, let $$R(m_1,m_2$$ label the rate of collision events that lead to a merging of
the two particles into a single new one.

Of course, in actuality not every collision leads to a merge event. For simplicity,
we will for now will assume that it does. As such, we can write

\begin{align*}
  R(m_1,m_2)=C(m_1,m_2)
\end{align*}

Later on, we will more carefully examine how the collision+sticking rate can be calculated.

[...also talk about definition of collision rate...]
[...also talk about "reaction rates"...]
[...difference between "collision", "collision+merge", "reaction" rates (fragmentation?)...]

[...collision+sticking rate, sticking probability, "Dust evolution w/ binning methods" eq.1.6-1.11...]

[...move the above elsewhere? -> "collisions" section?]
[...]

A kernel describing the stick-and-hit coagulation process can be written as
\begin{align*}
  K(m,m_1,m_2)=\frac{1}{2}\bigg[\delta_D(m-m_1-m_2)-\delta_D(m-m_1)-\delta_D(m-m_2)\bigg]\cdot R(m_1,m_2)
\end{align*}
Here, $\delta_D$ labels the Dirac delta function.

[...]

\newpage
** Numerical Integration with the Explicit Euler Scheme
\newpage
* Implementation of a Logarithmic Mass Axis Representation
** Introduction, Overview, & Goals for this Section
** Reasons for Making Use of a Logarithmic Representation
** Challenges Associated with a Logarithmic Representation
** Definition of the Logarithmic Mass Axis
** Implementation of the Kovetz-Olund algorithm
*** Reasons for the Necessity of the Kovetz-Olund Algorithm
Even in a highly simplified scenario, where only hit-and-stick coagulation
is included, the definition of the kernel $K_{kij}$ is not at all trivial.
To assure both the consistency and accuracy of the algorithm, one has to
take care of two separate problems, namely:
1. The conservation of mass /has/ to be assured, otherwise the numerical
   solution can not be assumed to remain stable for long. In the case of
   stick-and-hit coagulation, this means that for every pair of colliding
   particles, a single new particle has to be created. At the same time, the
   two initial particles have to be removed from the distribution. During this
   process, the total mass should remain unaffected down to machine precision.
2. When using a logarithmically spaced grid for the discretized mass axis,
   it can not be assumed that after a collision of two dust particles with
   masses $m_i$ and $m_j$ the resulting particle will carry a mass $m_k=m_i+m_j$
   whose value can be mapped trivially onto the grid. In general, the
   corresponding index will not be an integer, and instead lie between
   somewhere between the two neighboring grid points with indices $k$ and $k+1$.
   Therefore, the result of the merging of $m_i$ and $m_j$ has to be divided in
   some sensible way between these two neighboring bins.
*** Implementation of the Kovetz-Olund Algorithm
An elegant way for solving the two problems listed above is given in the paper
by [Kovetz & Olund, 1969], where they used the following procedure:
1. The stick-and-hit coagulation kernel is split into two parts. The first is
   the /gain/ of particles in bin $k$ due to the collision of particles from the
   bins $i$ and $j$. The second is the /loss/ of particles from bin $k$ due to
   collisions of particles in bin $k$ with particles from any other bin $j$.

   Using this separation into gain & loss, the dust particle mass distribution's
   temporal derivative can be expressed in the following form:
   \begin{align*}
     \deriv{n_k}{t}=\sum_{i,j}K_{kij}^\text{gain}n_in_j-\sum_jK_{kj}^\text{loss}n_kn_j
   \end{align*}
   In other words, the total kernel [from above, cite eq.] can be written as
   \begin{align*}
     K_{kij}=K_{kij}^\text{gain}-K_{ij}^\text{loss}\delta_{ki}
   \end{align*}
   Splitting the kernel like this into a gain & a loss term is a quite general
   approach, and can be used in more complex scenarios as well (including e.g.
   particle fragmentation processes).
2. For the scenario of pure hit-and-stick coagulation, a unique discretization
   of the kernel can be defined such that both the number of particles and the
   conservation of total mass are handled correctly. To do this, consider a
   pair of colliding particles with indices $i$ and $j$. Then, let the index
   $\bar k$ be chosen in such a way that the condition
   \begin{align*}
     m_{\bar k}\leq m_i+m_j<m_{\bar k+1}
   \end{align*}
   is satisfied.
3. As stated before, in hit-and-stick coagulation, a single new particle emerges
   for each pair of colliding particles. Using the definitions from above, this
   condition can be expressed as follows:
   \begin{align*}
     K_{\bar k,ij}^\text{gain}
     +K_{\bar k+1,ij}^\text{gain}
     \overset{!}{=}K_{ij}^\text{loss}
   \end{align*}
4. The second condition is that of mass conservation, which can be written as:
   \begin{align*}
     m_{\bar k}K_{\bar k,ij}^\text{gain}
     +m_{\bar k+1}K_{\bar k+1,ij}^\text{gain}
     \overset{!}{=}(m_i+m_j)K_{ij}^\text{loss}
   \end{align*}
5. Now, for the mapping of the resulting particle's mass onto two neighboring
   bins, let us define a parameter $\varepsilon$ such that
   \begin{align*}
     K_{\bar k,ij}^\text{gain}
     &=K_{ij}^\text{loss}\cdot(1-\varepsilon),\ \text{and}\\
     K_{\bar k+1,ij}^\text{gain}
     &=K_{ij}^\text{loss}\cdot\varepsilon
   \end{align*}
   This assures that [equation from pt 3] is satisfied. If we now plug this
   definition into [equation from pt 4] and solve for $\varepsilon$, we
   arrive at
   \begin{align*}
     \varepsilon:=\frac{m_i+m_j-m_{\bar k}}{m_{\bar k+1}-m_{\bar k}}
   \end{align*}
This is the algorithm of [Kovetz & Olund (1969)], and it was also used in subsequent
papers like [Brauer et al. (2008)] and [Birnstiel et al. (2010)].
# This discretization of the kernel is in detailed balance: For every two particles merging, these two particles are removed from the distribution, and precisely one par- ticle, with the proper mass, is inserted into the distribution. This detailed balance is not just guaranteed for the analytic continuous equation, but also for its numerical dis- cretization. This is important, because even the slightest deviation from this balance can cause large errors in the long-term.
\newpage
** Implementation of Near-Zero-Cancellation Handling for Coagulation
*** What is Near-Zero-Cancellation and where can it occur in general?
When using floating-point numbers following the representation defined
by the IEEE-754 standard, it can occur that
\begin{align*}
  a+b=a
  \ \ \ \ \ \text{for} \ \ \ \ \
  b\neq0
\end{align*}
Typically, this happens when
\begin{align*}
  |b|<\varepsilon_m\cdot|a|
\end{align*}
Here, $\varepsilon_m$ labels the /machine precision/.

[Add: How big is $\varepsilon_m$ for an f32, how big for an f64?]
*** Where can Near-Zero-Cancellation occur in our model?
Let $i$ and $j$ once again be the indices used to label two colliding
particles. Additionally, assume now that particle $i$ is /much smaller/ than
particle $j$.

The detailed balance approach from above requires the removal of both the
big and the small particle from the mass distribution, followed by the
re-insertion of a new particle carrying the initial pair's combined mass.
This new particle would then have a mass which is nearly identical to that
of the bigger one of the original two particles, it would be only a tiny
bit heavier.

In the approach defined above this would mean that $\bar k=j$, i.e. the
resulting particle will reside in the same bin as the larger original one.
Also, it would follow that $\varepsilon\ll1$.

Let us now take a look at the particle mass distribution in the bin $\bar k$
and, more specifically, by how much it changes from one timestep to the
next. For this particular pair of $i$ and $\bar k=j$, we can write:
\begin{align*}
  \deriv{n_{\bar k}}{t}
  =K_{\bar k,i\bar k}^{\text{gain}}n_in_{\bar k}
  -K_{\bar ki}^{\text{loss}}n_in_{\bar k}
\end{align*}
Plugging in [equation from above] leads to
\begin{align*}
  \deriv{n_{\bar k}}{t}
  =(1-\varepsilon)K_{\bar ki}^{\text{loss}}n_in_{\bar k}
  -K_{\bar ki}^{\text{loss}}n_in_{\bar k}
\end{align*}
Here, the two terms alsmost cancel each other out. What remains is a
contribution which is proportional to $\varepsilon$.

If $\varepsilon$ is small enough, the double-precision accuracy of the
floating point representation will lead to breakdown of the method.
[rewrite this sentence, copied almost exactly from Kees]
*** Handling Near-Zero-Cancellation for Stick-and-Hit Coagulation
It is relatively easy to identify the particle pairs $(i,j)$ for which
the scenario detailed above will occur. Let $i$ (without loss of generality)
be the index of the larger one of the two colliding masses. Cancellation
may then occur when the resulting $k$ is equal to $j$.

In that case, we carry out the subtraction in [previous equation]
analytically, and write:
\begin{align*}
  \deriv{n_{\bar k}}{t}
  =-\varepsilon K_{\bar ki}^{\text{loss}}n_in_{\bar k}
\end{align*}
[Elaborate on this, see "Dust Evolution with Binning Methods"]
\newpage
* Inclusion of Fragmentation Processes into the Kernel
** "First/Simple/Naive Attempt"
*** How to implement naive fragmentation?
- "Pulverization"
\newpage
** The MRN Distribution
- introduced by Mathis-Rumpl-Nordsieck, 1977
*** How to implement MRN distribution (for initialization)?

Particle mass distribution function is defined by the condition:
\begin{align*}
  \rho
    &=\int_{m_\text{min}}^{m_\text{max}}n(m)\cdot m\ \dd m
\end{align*}

Ansatz: power law with $q=-11/6$ (see `sizedistributions.pdf`)
\begin{align*}
  n(m)
    &\sim m^q\\
\end{align*}

Choose constant $A$ such that
\begin{align*}
  n(m)
    &=A\cdot m^q
\end{align*}

Plug into the integral from above
\begin{align*}
  \rho
    &=\int A\cdot m^q\cdot m\ \dd m\\
    &=\int A\cdot m^{q+1}\ \dd m\\
    &=\frac{A}{q+2}\ m^{q+2}\bigg|_{m_\text{min}}^{m_\text{max}}\\
    &=\frac{A}{q+2}\ (m_{\text{max}}^{q+2}-m_{\text{min}}^{q+2})
\end{align*}

Rearanging for $A$ leads to
\begin{align*}
  A
    &=\frac{(q+2)\cdot\rho}{m_\text{max}^{q+2}-m_\text{min}^{q+2}}
\end{align*}

Now we only need to choose values for $m_\text{min}$ and $m_\text{max}$.
- For the minimum mass, no lower limit exists. For it, the value
  corresponding to the smallest bin can be chosen [citation needed].
- For the maximum mass, an upper limit has to be chosen. It can be left
  as a free parameter to be set in `config.toml`, or set to something
  approximating $0.5\ \mum$. [see 2004 Dullemond & Dominik]
                ^ max. in interstellar medium (use for m_min?)

*** How to implement MRN distribution for fragmentation?

Let $m_i$ and $m_j$ be the masses of two particles that
are involved in a collision leading to fragmentation.

How do we model this?
- Distribute the mass $m_\text{tot}=m_i+m_j$ onto bins with $m<m_\text{tot}$.
- For this, use the MRN distribution $n(m)=Am^q$.
- For this, find the value of $A$.
- For this, use the formula from [Summarize: How to implement MRN distri].
- For this, find a value for $m_\text{max}$.

How do we choose a value for $m_\text{max}$?
- Can it be larger than $\text{max}(m_i,\ m_j)$ ?
  + "Some merging, some fragmenting"
  + "1 + 1 $\to$ 1.5 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1"
- If...
  + ...yes: It can at most be equal to $m_i+m_j$
  + ...no:  It can at most be equal to $\text{max}(m_i, m_j)$
- In either case, it can't really be /equal/ to either of the two values.
  + That would be stick-and-hit in case "yes", and bouncing in case "no".
  + It can at most be equal to the mass corresponding to the next-lower bin.

_Answer_:
- We don't really know.
- It doesn't /really/ matter that much anyhow.
- One could specify the method in `config.toml`, or just use one of the methods.

*** Test integration speed with MRN distribution enabled. Is there a slowdown?
** "Implementation of Near-Zero-Cancellation Handling for Fragmentation" (?)
\newpage
* Definition of the Dust Particle Reaction Rates
** Definition of the Dust Particle Reaction Rates
\newpage
** Definition of the Dust Particle Collision Rates

The collision rate for a pair of masses $m_1$ and $m_2$ can be written as

\begin{align*}
  C(m_1,m_2,\Delta v)=\sigma(m_1,m_2)\cdot\langle|\vec v_1-\vec v_2\rangle
\end{align*}
- ^ TODO: Is there not a "number density" term missing here?
    Otherwise, the unit would be `m^3 / s`.

Here, $\sigma(m_1,m_2)$ labels the cross section for a collision event. The
relative velocity between such two particles is given by the difference $\vec v_1-\vec v_2$,
its absolute value is $|\vec v_1-\vec v_2|$, and $\langle|\vec v_1-\vec v_2|\rangle$ is
the expectation value of that quantity.

[Simplification was done here]
[Elaborate: Give more precise formula]

In our notation:

\begin{align*}
  C_{ij}=\sigma_{ij}\cdot\langle\Delta v_{ij}\rangle
\end{align*}

\newpage
** Definition of the Collision Cross Section

The cross section for a collision of two particles $i$ and $j$
with radii $a_i$ and $a_j$ can be written as

\begin{align*}
  \sigma_{ij}=\pi\cdot(a_i+a_j)^2
\end{align*}

Here, the radii can be computed from the corresponding masses $m_i$ and $m_j$
from equation [Summarize: Assumptions needed for the (naive/simple) particle mass distribution.].

[Add plot here]

\newpage
** Definition of the Relative Particle Velocity
\newpage
* Calculation of the Relative Dust Particle Velocities
** Definition of the Relative Velocity due to Radial Drift
\begin{align*}
  \Delta u_\text{RD}=\big|u_r(m_1)-u_r(m_2)\big|
\end{align*}

dust radial velocity:
\begin{align*}
  u_r=\frac{u_g}{1+\text{St}^2}-\frac{2u_n}{\text{St}+\text{St}^{-1}}
\end{align*}

gas radial velocity:
\begin{align*}
  u_g=-\frac{3}{\Sigma_g\sqrt{r}}\cdot\del_r\big(\Sigma_g\nu_g\sqrt{r}\big)
\end{align*}

particle maximum drift velocity:
\begin{align*}
  u_n=-\frac{E_d}{2\rho_g\ \Omega_K}\cdot\pderiv{P_g}{r}
\end{align*}
#+ATTR_LATEX: :width \textwidth
[[../../msc-thesis.py/figures/22/dv_RD.pdf]]
\newpage
** Definition of the Relative Velocity due to Differential Settling
\begin{align*}
  \Delta u_\text{DS}=\big|u_i-u_j\big|
\end{align*}
\begin{align*}
  u_i=ts\cdot\Omega_K^2\cdot z
\end{align*}
\begin{align*}
  \text{St}=\Omega_K\cdot t_s
\end{align*}
\begin{align*}
\end{align*}
#+ATTR_LATEX: :width \textwidth
[[../../msc-thesis.py/figures/22/dv_DS.pdf]]
\newpage
** Definition of the Relative Velocity due to Brownian Motion
\begin{align*}
  \Delta u^\text{BR}_{ij}
     &=\sqrt{\frac{8k_BT}{\pi}\cdot\frac{m_i+m_j}{\ m_i\cdot m_j}}\\
     &=\sqrt{\frac{8}{\pi\beta\mu_{ij}}}
\end{align*}
#+ATTR_LATEX: :width \textwidth
[[../../msc-thesis.py/figures/22/dv_BR.pdf]]
\newpage
** Definition of the Relative Velocity due to Turbulence
#+ATTR_LATEX: :width \textwidth
[[../../msc-thesis.py/figures/22/dv_TU.pdf]]
\newpage
** Definition of the Azimuthal Relative Velocity
\begin{align*}
  \Delta u_{ij}
  =\left|u_n\cdot\left(
    \frac{1}{1+\text{St}_i^2}-\frac{1}{1+\text{St}_j^2}
  \right)\right|
\end{align*}
#+ATTR_LATEX: :width \textwidth
[[../../msc-thesis.py/figures/22/dv_AZ.pdf]]
\newpage
** Total Relative Velocity
\begin{align*}
  \Delta u_{ij}=\Delta u_\text{BR}+\Delta u_\text{RD}+\Delta u_\text{AZ}+\Delta u_\text{TU}+\Delta u_\text{DS}
\end{align*}
\begin{align*}
  \Delta u_{ij}=\sqrt{\Delta u_\text{BR}^2+\Delta u_\text{RD}^2+\Delta u_\text{AZ}^2+\Delta u_\text{TU}^2+\Delta u_\text{DS}^2}
\end{align*}
#+ATTR_LATEX: :width \textwidth
[[../../msc-thesis.py/figures/22/dv_tot.pdf]]
\newpage
* Implemention of Stochastic Kernel Sampling
\newpage
* Summary of Results
\newpage
* Thesis Discussion & Outlook
\newpage
* Appendix
** Glossary
a \cite{mathis_rumpl_nordsieck_1977}
b \cite{dullemond_dominik_2004}
c \cite{brauer_dullemond_henning_2008}
d \cite{birnstiel_dullemond_brauer_2010}
* +References+
\bibliographystyle{plain}
\bibliography{./bibliography/ref.bib}
